\documentclass{INTERSPEECH2023}

% **************************************
% *    DOUBLE-BLIND REVIEW SETTINGS    *
% **************************************
% Comment out \interspeechcameraready when submitting the 
% paper for review.
% If your paper is accepted, uncomment this to produce the
%  'camera ready' version to submit for publication.
\interspeechcameraready 

% imports
\usepackage{bibentry}
\usepackage[acronym,toc,shortcuts,nonumberlist]{glossaries}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{siunitx}
\usepackage{xcolor}

\nobibliography*  % used together with bibentry package to allow footnote bibentry citations

\input{acronyms}
\input{commands}

\title{Comparative Analysis of the wav2vec 2.0 Feature Extractor}
\name{Peter Vieting$^{1}$, Ralf Schl\"uter$^{1,2}$, Hermann Ney$^{1,2}$}
\address{
  $^1$RWTH Aachen University, Germany\\
  $^2$AppTek GmbH, Germany}
\email{\{vieting,schlueter,ney\}@cs.rwth-aachen.de}

\begin{document}

\maketitle
 
\begin{abstract}
Raw waveform modeling is an interesting research topic for \gls{ASR}.
\wvtwo has seen large success recently and makes use of a convolutional \fe which operates directly on the speech waveform.
Typically, this \fe is seen as part of the overall architecture, but this work investigates this component specifically.
It is used as a replacement for classical feature extraction methods in a \gls{CTC} setup and compared to an alternative neural \fe approach.
Finally, we analyze the learned representations of both neural \fes.
\end{abstract}
\noindent\textbf{Index Terms}: speech recognition, feature extraction, raw waveform modeling


\section{Introduction}
\glspl{AM} for \gls{ASR} traditionally makes use of hand-designed feature extraction methods like log Mel filterbank features \addref or Gammatone features \cite{schlueter:icassp07}.
These techniques are motivated by insights into the properties of the human auditory system.
However, their fixed design inevitably leads to information loss during the feature extraction.

This issue can be addressed by neural \fes.
Since their parameters are learned during training, these \fes can adapt to the need of \glspl{AM} and the loss of particularly helpful information may be avoided.
A number of works have been proposed in this direction, often using convolutional \glspl{NN} \addref but also applying parametrized functions \cite{ravanelli2018sincnet} and other architectures \cite{sainath2015cldnn}.

Recently, the \wvtwo framework \cite{facebook2020wav2vec2} has received large attention.
It is mainly known for its contrastive self-supervised pre-training, which allows to pre-train a model on large amounts of unlabeled audio data.
Subsequently, it can be fine-tuned for a specific downstream task at hand.
While the context network with multiple \transformer blocks constitutes the major part of the architecture, the \wvtwo model also makes use of a convolutional feature encoder.
\TODO{feature encoder vs. \fe here.}

While the \wvtwo \fe is similar to other neural feature extraction methods, it is not yet closely studied in the literature.
Choi et al. study the feature encoder\citearxiv{choi2022w2v2fe}.

\draft{
Comparable to other neural feature extraction methods, but not closely studied in the literature.
Maybe some references or name some works which are similar.

Highlight contributions of this paper.
}

\section{Related work}
\draft{
Works on raw waveform acoustic modeling.
Think about how to properly reference \cite{vieting2021waveform} without disclosing author information.

Different \wv papers \cite{facebook2019wav2vec, facebook2019vqwav2vec, facebook2020wav2vec2, facebook2020xlsr}.
Describe differences between the approaches.
Besides modifications of the self-supervised training criteria -- moving from the future time step prediction in \cite{facebook2019wav2vec} to masked time step prediction -- and incorporating quantization modules, also the architecture has been revised.
Fully convolutional vs. \transformer.
Differences in \fe.

\cite{facebook2019wav2vec} uses 5 layers with kernel sizes (10, 8, 4, 4, 4) and strides which are always exactly half the kernel size.
All have 512 channels, a group normalization layer with a single group and a \relu nonlinearity.
\cite{facebook2020wav2vec2} uses seven layers with kernel sizes (10, 3, 3, 3, 3, 2, 2), strides (5, 2, 2, 2, 2, 2, 2), and a GELU activation function.
The first layer applies group normalization and finally, layer normalization and a linear projection are added.
}

\section{Methods}
We always normalize the waveform to zero mean and unit variance.
Then, it is input to a \fe.
The \fe is followed by the remaining \gls{AM}.
While the separation between \fe and remaining \gls{AM} is not sharp for neural feature extraction, we always refer to the part that replaces the traditional feature extraction as \fe.

\subsection{Feature Extractors}
\subsubsection{Standard Feature Extraction}
As a reference, we use standard, hand-designed \fes -- namely log Mel filterbank features and Gammatone features.
To compute the log Mel filterbank features, first the \gls{STFT} of the waveform is computed with a window size of \ms{25} and a shift of \ms{10}.
We keep the square of the magnitude and perform Mel warping to obtain a 80-dim vector.
Finally, $\log_{10}$ and normalization are applied.

Gammatone features apply a gammatone filterbank to the pre-emphasized speech signal, perform temporal integration over each channel using a low pass filter, i.e., the Hanning window with a size of \ms{25} and a shift of \ms{10}, compress using the $10^{th}$ root and finally compute the \gls{DCT} of the values \cite{schlueter:icassp07}.
Further, the the resulting 50-dim features are normalized.

\subsubsection{\wvtwo Feature Extractor}
The \gls{FE} of the \wvtwo model \cite{facebook2020wav2vec2} mainly consists of 7 convolutional layers.
They are configured with kernel sizes (10, 3, 3, 3, 3, 2, 2), strides (5, 2, 2, 2, 2, 2, 2), and a GELU activation function.
The first layer applies group normalization and finally, layer normalization and a linear projection is added.
The total receptive field of the \wvtwo \fe is \ms{9999} and the shift between consecutive frames is \ms{20}.
Since the other \fes operate at a shift of \ms{10} and the \wvtwo \fe is built in a modular way, we remove the last layer with a stride of 2 in order to achieve features at the same frame rate.
This reduces the total receptive field to \ms{9999}.

\subsubsection{Supervised Convolutional Features}
As an alternative comparable neural feature extraction method, we use \gls{SCF} \cite{tuske2018:waveform}.
Similarly, a convolutional filterbank with learnable parameters is applied to the waveform.
As in the case of Gammatones, a temporal integration is performed over each channel.
However, in this case multiple filters are used for temporal integration allowing for multi-resolutional processing.
Additionally, these filters are learned during training.
We set the size of the first filterbank to 256, its strides to 10 and the number of channels to 150.
The second convolutional filter applies 5 temporal integration filters with a size of 40 and strides of 16 each.
Since the output of all 5 filters is stacked, we have a resulting feature dimension of 750.
The total receptive field of the \gls{SCF} is \ms{9999}.

\section{Experiments}
\subsection{Model Sizes}
By default, I use 40k parameters for i6 features compared to 4M for \wvtwo.
Can we improve the performance by making the model larger?
\subsubsection{wav2vec 2.0 Components}
To understand where \wvtwo uses the parameters and how much they contribute to the \gls{FE}'s performance, we run it with different configurations.
The results for different widths and depths are shown in \refTab{table:features_w2v_size}.
% Kernel sizes and strides were chosen as (10, 3, 3, 3, 3, 2), (5, 2, 2, 2, 2, 2) for 6 layers, (10, 6, 3, 3, 3), (5, 4, 2, 2, 2) for 5 layers, (10, 6, 6, 3), (5, 4, 4, 2) for 4 layers, (20, 6, 6), (10, 4, 4) for 3 layers, (32, 20), (16, 10) for 2 layers and (320), (160) for 1 layer.
While a larger inner dimension leads to a better performance, no major effect can be observed regarding the number of layers.
The final projection additionally improves the performance, see \refTab{table:features_w2v_proj}.
We can additionally vary the kernel size and final norm.

\subsubsection{Inner Dimensions}
As a larger inner dimension clearly showed better performance for the \wvtwo \gls{FE}, we increase the inner dimensions of the i6 features as well.
However, as shown in \refTab{table:features_i6_size}, neither increasing the inner dimension of the first layer nor the second layer helps to improve the performance.

\subsubsection{First Layer Window Size}
What role does the size of the first layer play? Is it important to cover one pitch period (max. fundamental period)?
By default, I use a window size of \ms{16}.
Experiments on using different window sizes in \refTab{table:features_window_size} suggest that a larger window size has a detrimental effect while going from the default \ms{16} down to \ms{10} shows a marginal improvement.

\section{Results}
The results for the different feature extractors used with a competitive \gls{CTC} model on Librispeech are shown in \refTab{table:features_general}.
\input{tables/features_general}

\subsection{Model Sizes}
Results are in the following tables.
\input{tables/features_w2v_size}
\input{tables/features_w2v_proj}
\input{tables/features_i6_size}
\input{tables/features_window_size}

\subsection{\wvtwo Pre-Training}
\input{tables/features_pretraining}

\subsection{Frequency Response of Learned Filters}
\input{figures/first_layer}

\section{Conclusions}

\section{Acknowledgements}

\ifinterspeechfinal
     Personal information revealing author identity
\else
     Anonymized information not revealing author identity
\fi

{
\color{red}
\section{To Dos/Questions}
\begin{itemize}
  \item \wvtwo feature extractor vs. feature encoder
\end{itemize}
}

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}
